D/B supervised and unsupervised learning-
The target data is not labeled in unsupervised learning. But in supervised learning, target is labelled.
Both regression and classification comes under Supervised learning.

The unsupervised learning algorithms are-
Clustering-- Points close form clusters. The cluters create a groupof data with the help of which,a labelled data is created.
Two widely used techniques are Kmeans clustering and hierarchical clustering.

1.Kmeans Clusteing:- Here the value k stands for no. of clusters.
Let's say there are datapoints distributed in such a way,that there are 2 clusters.Now, because there are two clusters, take 2 centeroids.
Now,calc distance of each centeroid from these data points. This distance canbe calculated with the helpof euclidean distance or manhattan distance.
This is itertion no. 1.
Now,next what will happen is centeroids willnow try to shift tothe center of the cluster. And we will have new location of centeroids.
Then, we will repeat above steps again.And will continue till data points reassignment is no more required. We have external loop also for random
assignments of centeroids.

https://drive.google.com/drive/u/0/folders/1YNR7Exl_YW-m6hgY2lBClLmJ6_HPgf5E

Now,to select which clustering is better , we see variance within the cluster and variance between the cluster.Within should be low, between should be high,
.This is the property for selecting good cluster.


The variance within - is alsoknown as Inertia or Within Cluster sum of squares or Inter Distance. This should be low.
The variance between is Intra distance and this should be high.

Now selecting best value ofk:- Elbow method is used.

Clustering.ipynb---------------------------------------------------------------

-------------------------

Hierarchical_Clustering.ipynb----- hierarchy of clusters.

---for seaborn.ipynb--------------------------------------

https://www.kaggle.com/code/shubhamsinghgharsele/exploratory-data-analysis-on-automobile-dataset

# following are the attriputer that show positive correlation

plt.figure(figsize = (12,8))

sns.heatmap(auto_df.corr(),annot = True, cmap='Blues', mask = (auto_df.corr() <= 0.7 ))
plt.show()


# following are the attriputer that show negative correlation

plt.figure(figsize = (12,8))

sns.heatmap(auto_df.corr(),annot = True, cmap='Blues', mask = (auto_df.corr() >= 0 ))
plt.show()

----------------------------------

Hierarchical clustering---------------

---------------------------------------

In hierarchical clustering each and every data point will measure it's distance from the other data point. Here we donot have any concept of centeroid.

-**draw some point and see which data points are closer. Cluster near data points and create dendogram.
The dendogram helps in creating clusters. Draw a horizontal line on dendograms , to decide how many clusters we want to form.
The two types of hierarchical clustering are Agglomerative clustering- means bottom up approach
2nd is Divisive Clustering, -top to bottom approach. i.e. We keep on dividing big cluster into smaller clusters.

Now, which is better? Kmeans or hierarchical? It totally depends.

Hierarchical_Clustering.ipynb----

-------------------------------------------
PCA----------------------------------------
-------------------------------------------

PCA - Principal Component Analysis.

This clustering will be done in 2 parts. The 2nd part will be built on 1st part, which is Kmeans clustering.

PCA is a Dimensionality Reduction Algorithm. Now, what si dimensionality and why do we have to do dimensionality reduction?
https://drive.google.com/drive/u/2/folders/1YNR7Exl_YW-m6hgY2lBClLmJ6_HPgf5E --------- when u have two features, analysis becomes easy because u can visualize
it. With various 1 D plots, 2 D plots- bar graph,scatter plot, etc and u can understand yr data. If we add 1 more dimension, then it becomes 3 D, and still 
u can visualize. But what if u have 7 features, that means 7 dims, then it becomes difficult to visualize. So we reduce 7 D to 3 D, then these 3 features
are known as principal component, which is a linear combination of features.

Then, pc1=theta1*x1+theta2*x2+theta3*x3+theta4*x4+theta5*x5+theta6*x6+theta7*x7 , where theta are some coefficient value.
pc2=theta1*x1+theta2*x2+theta3*x3+theta4*x4+theta5*x5+theta6*x6+theta7*x7
pc3=theta1*x1+theta2*x2+theta3*x3+theta4*x4+theta5*x5+theta6*x6+theta7*x7

Now, the thing to remember is Even when we are reducing the dimensions, we should must not loose the information.

The no. of PC depends on no. of features.

https://drive.google.com/drive/u/2/folders/1YNR7Exl_YW-m6hgY2lBClLmJ6_HPgf5E------PCA explanation

AB^2=BC^2+AC^2

Now, BC is the distance that we are trying to minimize.

Now, distance AB is fixed, so what we can do is minimize BC and maximize AC. That is , shift the line towards the data point. And thats how we find the best fit line.
And this is our PC1. Now, the second PC is always 90degree or perpendicular to 1st one.

One of the best part of PCA is there is no multicollinearity (i.e. no correlation) between PCs.

  



PCA.ipynb--------


-----------
Association rules and support and confidence

support=no.of antecedent and consequent/total no.of trxs------- tells us how popular the pair ofproduct are.

Now,lets say we have 2 shops--


shop1 sells-
m,s
m,s
m,chips
m,cheese
m,cold drinks

shop2 sells-
m,s
m,s
m,chips
bread,cheese
chips,colddrink

if maggi then sauce---->shop1 support=2/5,shop2 support=2/5

but this pair,m and s has more popularity in shop2 compare to shop1.

Support matrix has its own drawback.That is why, we will go for confidence matrix.

shop1 confidence=2/5,shop2 confidence=2/3 

confidence= no.of antecedent and consequent/no.of times antecedent

m,s
m,c
b,b
m,s
b,c
pasta,s
pasta,sauce
p,s,cheese
p,s,cheese

support maggi sauce=2/9
confidence=2/3

But the issue with confidence,is,we are not taking care of the consequent.And is u lookinto the data,sauce is more popular with pasta rather than maggi.
That is,consequent is more popular with different antecedent. This is one ofthe limitations of confidence formula.
So, that is why, we go for 

lift= no.of antecedent and consequent purchased together/no.of times antecedent has been purchased*no.of times consequent has been purchased

for maggi and sauce,lift=2/3*6=1/9=0.11

i.e. for this particular pair,lift value is bad,because sauce is more popular with pasta then maggi.

liftvalue for pasta and sauce=4/4*6=1/6

Now, right now,wehave 9 trxsbut in reality, there are lakhs of trx. Froma single trx, we can have many association rules.So for lakhs of trxs,

we use Apriori Algorithm-by r.agarwal and r shrikant.------------Support_Confidence.png------in folder----------

1.Goforthe individual product and write their support.

beer,wine,cheese,eggs,flour,potatochips. In total there are 20 trxs.
support for beer=8/20, wine=8/20,cheese=8/20, eggs=7/20,flour=5/20,butter=6/20,potato chips=10/20

Now, we will eradicate all the unpopular product with the helpof a threshold,let's sy my threshold support value is 7/20.Tf,flour and butter are discarded.
Then,no.of association rules decrease.

Now,items discarded, dont consider them. Now, creating pair of products.

beer and wine,beer cheese,beer eggs,beer potato chips, wine cheese,wine eggs,wine potato chips,cheese eggs,cheese potato chip,eggs potatochips.

Now,calculating support for these pairs.
beer and wine-2/20
beer cheese-2/20
beer eggs-2/20
beer potato chips-9/20
wine cheese
wine eggs 
wine potato chips
cheese eggs
cheese potato chip
eggs potatochips.

Now, discarding unpopular pairs.

Now,in the next iteration repeat this process with 3 products,then with 4, till all done. And like this we will deal with just the popular products.
Run above steps with different thresholdds, select the one which give satisfactory result.

--------------
Association_Rules.ipynb-------------GroceryStoreDataset.ipynb
--------------
In the retail domain,association rules are very important.Apart from that domain,We can apply the association rules with patients also.
like cold ,fever,headache,vomiting then what could be the reason.
lly, another ex. is when people order something in restaurants.U can do research and increase productivity.

------------------------------------------

Network Analytics
-------------------------------------------
In a network, we have some entiites known as nodes.These are connected with each other.So, we cantell that, nodes can be called as vertex, and they are connected
through edges.Give some name to these vertex.
v=(v1,v2,v3,v4,v5)
e={(v1,v2),(v1,v4),(v2,v3)......}
So this network analysis is same as Graph analysis.

Now, we have directed graph too, in which commumnication happens in the specific directions.

What we will concentrate is on a two-way graph. and see the use of Network analysis. First, ae these nodes always computers?
No, node can be some social media account like twitter,facebook,itcan be an airport. So we can say that this is a network of some entities.

How do u know, how strong a specific node is? Degree centrality ,closeness centrality,betweeness centrality.

https://drive.google.com/drive/u/0/folders/1YNR7Exl_YW-m6hgY2lBClLmJ6_HPgf5E

Degree Centrality-is direct connections every node is having.Only checks the no.of direct connections.

Closeness Centrality-How close specific nodes are from other nodes. 1/total distance from all nodes.

Betweenness Centrality-Togofrom1 node to another, the shortest path that u will follow.

NetworkAnalysis.ipynb--------------------------------------

routes.csv-------------------incomplete------------

-----------------------------------------------------------------------------------------------------------------------------------------------------










 






 




 














